{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53600f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b954e",
   "metadata": {},
   "source": [
    "###  This code is adapted from METIS example scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51c7fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Box Office Mojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and returns the string in\n",
    "    the next sibling object (the value for that attribute) or None if nothing is found.\n",
    "    '''\n",
    "    \n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    \n",
    "    if not obj: \n",
    "        return None\n",
    "    \n",
    "    # this works for most of the values\n",
    "    next_element = obj.findNext()\n",
    "    \n",
    "    if next_element:\n",
    "        return next_element.text \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def to_float(string_number):\n",
    "    if string_number == '-':\n",
    "        return float('nan')\n",
    "    elif string_number =='':\n",
    "        return float('nan')\n",
    "    else: \n",
    "        return float(string_number.replace(',',''))\n",
    "    \n",
    "def to_int(string_number):\n",
    "    if string_number.isdigit() == True:\n",
    "        return float(string_number)\n",
    "    else: \n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a39d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    if moneystring == None:\n",
    "        return float('Nan')\n",
    "    else:\n",
    "        moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "        return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_date(datestring):\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0efdc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_dict(link):\n",
    "    #start_time = time.sleep(1)\n",
    "    '''\n",
    "    From BoxOfficeMojo link stub, request movie html, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - title\n",
    "        - domestic gross\n",
    "        - runtime \n",
    "        - MPAA rating\n",
    "        - full release date\n",
    "    Return information as a dictionary.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.boxofficemojo.com'\n",
    "    \n",
    "    #Create full url to scrape\n",
    "    url = base_url + link\n",
    "    \n",
    "    #Request HTML and parse\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "    \n",
    "    headers = ['movie_title', 'domestic_total_gross','international_gross',\n",
    "               'runtime_minutes', 'rating', 'release_date',\n",
    "              'distributor','budget','opening','genres','widest_release','opening_rank',\n",
    "              'imdb_user_score','metascore','rating_dict', 'rating_dist_dict']\n",
    "    \n",
    "    #Get title\n",
    "    title_string = soup.find('title').text\n",
    "    title = title_string.split('-')[0].strip()\n",
    "\n",
    "    #Get domestic gross\n",
    "    raw_domestic_total_gross = (soup.find(class_='mojo-performance-summary-table')\n",
    "                                    .find_all('span', class_='money')[0]\n",
    "                                    .text\n",
    "                               )\n",
    "    domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "    #Get international gross\n",
    "    raw_international_total_gross = (soup.find(class_='mojo-performance-summary-table')\n",
    "                                    .find_all('span', class_='money')[1]\n",
    "                                    .text\n",
    "                               )\n",
    "    international_total_gross = money_to_int(raw_international_total_gross)\n",
    "    \n",
    "    #Get runtime\n",
    "    raw_runtime = get_movie_value(soup,'Running')\n",
    "    if raw_runtime == None:\n",
    "        runtime = float('nan')\n",
    "    else:\n",
    "        runtime = runtime_to_minutes(raw_runtime)\n",
    "    \n",
    "    #Get rating\n",
    "    rating = get_movie_value(soup,'MPAA')\n",
    "\n",
    "    #Get release date\n",
    "    raw_release_date = get_movie_value(soup,'Release Date').split('\\n')[0].split('(')[0]\n",
    "    release_date = to_date(raw_release_date)\n",
    "\n",
    "    #Get distributor\n",
    "    distributor_raw = get_movie_value(soup,'Distributor')\n",
    "    remover = soup.find(text=re.compile('Distributor')).find_next().find('a')\n",
    "    if remover == None:\n",
    "        distributor = distributor_raw\n",
    "    else:\n",
    "        distributor = distributor_raw.removesuffix(remover.text)\n",
    "    \n",
    "    #Get Budget\n",
    "    raw_budget = get_movie_value(soup,'Budget')\n",
    "    \n",
    "    budget = money_to_int(raw_budget) \n",
    "    \n",
    "    #Get Opening\n",
    "    raw_opening = get_movie_value(soup,'Opening')\n",
    "    if raw_opening == None:\n",
    "        opening = float('nan')\n",
    "    else:\n",
    "        opening = money_to_int(raw_opening.removesuffix('\\n            theaters'))\n",
    "    \n",
    "    #Get genres\n",
    "    \n",
    "    genres_raw = get_movie_value(soup,'Genres')\n",
    "    if genres_raw == None:\n",
    "        genres = [float('nan')]\n",
    "    else:\n",
    "        genres = genres_raw.replace(' ','').split('\\n\\n')\n",
    "    \n",
    "    #get widest release\n",
    "\n",
    "    raw_widest_release = get_movie_value(soup,'Widest Release').removesuffix(' theaters')\n",
    "    widest_release = money_to_int(raw_widest_release)\n",
    "\n",
    "    \n",
    "    #Opening Rank \n",
    "    if len(soup.find(text=re.compile('Rank')).find_next().find_all('a'))>20:\n",
    "        opening_rank = int(soup.find(text=re.compile('Rank')).find_next().find_all('a')[6].find_next().text)\n",
    "    else:\n",
    "        opening_rank = float('nan')\n",
    "    \n",
    "    #Request HTML and parse from IMDB\n",
    "    imdbpro_url = soup.find(text=re.compile('IMDbPro')).find_next().find('a')['href']\n",
    "    imdb_url = imdbpro_url.replace('pro','www')\n",
    "\n",
    "    imdb_response = requests.get(imdb_url)\n",
    "    imdb_page = imdb_response.text\n",
    "    imdb_soup = BeautifulSoup(imdb_page,\"lxml\")\n",
    "    \n",
    "    #IMDb User Score\n",
    "    if imdb_soup.find(text=re.compile('IMDb RATING')).find_next() == None:\n",
    "        imdb_user_socre = float('nan')\n",
    "    elif imdb_soup.find(text=re.compile('IMDb RATING')) == None:\n",
    "        imdb_user_socre = float('nan')\n",
    "    else:\n",
    "        imdb_user_score = float(imdb_soup.find(text=re.compile('IMDb RATING')).find_next().text.split('/')[0])\n",
    "    \n",
    "    #Metascore\n",
    "    metascore = to_int(imdb_soup.find(text=re.compile('Metascore')).find_previous().find_previous().text)\n",
    "    \n",
    "    # Request HTML and parse from IMDB User Ratings\n",
    "    imdb_rating_url = imdb_url.split('?')[0]+'/ratings/'\n",
    "    imdb_rating_response = requests.get(imdb_rating_url)\n",
    "    imdb_rating_page = imdb_rating_response.text\n",
    "    imdb_rating_soup = BeautifulSoup(imdb_rating_page,\"lxml\")\n",
    "    \n",
    "    rating_dict = {}\n",
    "    rating_dict['all_all'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[1]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[1]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['all_<18'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[2]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[2]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['all_18-29'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[3]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[3]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['all_30-44'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[4]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[4]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['all_45+'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[5]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[5]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    " \n",
    "    rating_dict['men_all'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[7]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[7]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['men_<18'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[8]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[8]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['men_18-29'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[9]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[9]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['men_30-44'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[10]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[10]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['men_45+'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[11]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[11]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    \n",
    "    rating_dict['women_all'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[13]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[13]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['women_<18'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[14]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[14]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['women_18-29'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[15]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[15]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['women_30-44'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[16]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[16]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['women_45+'] = [to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[17]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[1]\n",
    "                                 .find_all('td')[17]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    " \n",
    "    \n",
    "    rating_dict['top_users'] = [to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[0]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[0]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['US_users'] = [to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[1]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[1]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    rating_dict['non_US_users'] = [to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[2]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[0:3]),\n",
    "                              to_float((imdb_rating_soup.find_all('table')[2]\n",
    "                                 .find_all('td')[2]\n",
    "                                 .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0',''))[3:])]\n",
    "    \n",
    "    rating_dist_dict = {}\n",
    "    top = 10\n",
    "    for x in imdb_rating_soup.find_all('table')[0].find_all('td')[2::3]:\n",
    "        rating_dist_dict[top] = (money_to_int(x .text.replace('\\n','')\n",
    "                                 .replace(' ','')\n",
    "                                 .replace('\\xa0','')))\n",
    "        top = top -1\n",
    "\n",
    "        \n",
    "    #Create movie dictionary and return\n",
    "    movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                international_total_gross,\n",
    "                                runtime,\n",
    "                                rating, \n",
    "                                release_date,\n",
    "                                distributor,\n",
    "                                budget,\n",
    "                                opening,\n",
    "                                genres,\n",
    "                                widest_release,\n",
    "                                opening_rank,\n",
    "                                imdb_user_score,\n",
    "                                metascore,\n",
    "                                rating_dict,\n",
    "                                rating_dist_dict]))\n",
    "    return movie_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "140f0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(start_url):\n",
    "    time.sleep(1)\n",
    "\n",
    "    response_s = requests.get(start_url)\n",
    "    page_s = response_s.text\n",
    "\n",
    "    soup_s = BeautifulSoup(page_s,\"lxml\")\n",
    "\n",
    "    table = soup_s.find('table')\n",
    "\n",
    "    links = []\n",
    "    for row in table:\n",
    "        if len(row.find_all('td')) == 0:\n",
    "            pass\n",
    "        elif row.find_all('td')[2].text =='Limited':\n",
    "            pass\n",
    "        else:\n",
    "            links.append(row.find_all('td')[0].find('a')['href'].split('?')[0])\n",
    "    return(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a4000901",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heres where we start getting URLS\n",
    "\n",
    "year = 2010\n",
    "url_list = []\n",
    "while year<2020:\n",
    "    month = 1\n",
    "    while month <13:\n",
    "        url_list.append('https://www.boxofficemojo.com/calendar/'+str(year)+'-'+str(month)+'-01/')\n",
    "        month = month+1\n",
    "    year = year+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acc54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for x in url_list:\n",
    "    links = links+get_links(x)\n",
    "links = list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d17b373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1672"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bd0d503f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/release/rl3330246145/\n",
      "/release/rl1342408193/\n",
      "/release/rl1618707969/\n",
      "/release/rl1027966465/\n",
      "/release/rl3410920961/\n",
      "/release/rl4266296833/\n",
      "/release/rl1566606849/\n",
      "/release/rl2002617857/\n",
      "/release/rl493389313/\n",
      "/release/rl3260843521/\n",
      "/release/rl2051507713/\n",
      "/release/rl3540157953/\n",
      "/release/rl939755009/\n",
      "/release/rl21464577/\n",
      "/release/rl1560839681/\n",
      "/release/rl2214823425/\n",
      "/release/rl805602817/\n",
      "/release/rl1191806465/\n",
      "/release/rl3011282433/\n",
      "/release/rl1497662977/\n",
      "/release/rl2186249729/\n",
      "/release/rl275351041/\n",
      "/release/rl324240897/\n",
      "/release/rl122652161/\n",
      "/release/rl369657345/\n",
      "/release/rl1887340033/\n",
      "/release/rl2773255681/\n",
      "/release/rl1298826753/\n",
      "/release/rl1265468929/\n",
      "/release/rl1046382081/\n",
      "/release/rl1216185857/\n",
      "/release/rl2105378305/\n",
      "/release/rl3862988289/\n",
      "/release/rl2893252097/\n",
      "/release/rl2373682689/\n",
      "/release/rl486835713/\n",
      "/release/rl3081142785/\n",
      "/release/rl256542209/\n",
      "/release/rl1985119745/\n",
      "/release/rl3915417089/\n",
      "/release/rl3044115969/\n",
      "/release/rl67274241/\n",
      "/release/rl4165764609/\n",
      "/release/rl1828750849/\n",
      "/release/rl3305539073/\n",
      "/release/rl4014048769/\n"
     ]
    }
   ],
   "source": [
    "### Uncomment for restarting list\n",
    "movie_data = []\n",
    "\n",
    "#x = len(movie_data)\n",
    "for link in links:#[x-1:]:\n",
    "    movie_data = [get_movie_dict(link)]+movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "262ddc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_data.pkl', 'wb') as f:\n",
    "    pickle.dump(movie_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51a76b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Debugger Window\n",
    "\n",
    "base_url = 'https://www.boxofficemojo.com'\n",
    "link = '/release/rl2724955649/'\n",
    "\n",
    "#Create full url to scrape\n",
    "url = base_url + link\n",
    "\n",
    "#Request HTML and parse\n",
    "response = requests.get(url)\n",
    "page = response.text\n",
    "soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "len(soup.find(text=re.compile('Rank')).find_next().find_all('a'))#.find_next().find_all('a')[6].find_next().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4982d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
